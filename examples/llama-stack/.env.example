# Llama Stack server (here using a local deployment)
BASE_URL= "http://localhost:8321"

# Generative AI
INFERENCE_MODEL="ollama/llama3.2:3b-instruct-fp16"
TEMPERATURE="0.0"
TOP_P="0.95"
MAX_TOKENS="4096"
STREAM="False"
USE_PROMPT_CHAINING= "True"

# MCP-related
DOCLING_MCP_URL= "http://host.containers.internal:8000/mcp"
