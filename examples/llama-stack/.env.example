# Llama Stack server (here using a local deployment)
BASE_URL= "http://localhost:8321"

# Generative AI
INFERENCE_MODEL="meta-llama/Llama-3.2-3B-Instruct"
TEMPERATURE="0.0"
TOP_P="0.95"
MAX_TOKENS="4096"
STREAM="False"
USE_PROMPT_CHAINING= "True"

# MCP-related
DOCLING_MCP_URL= "http://host.containers.internal:8000/sse"
